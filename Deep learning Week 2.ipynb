{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic regression is algorithm for binary classification\n",
    "- use y to output image\n",
    "- use each input feature vector, x has all pixels and color\n",
    "- Notation\n",
    "- (x,y ) x is dimensional feature vector and y is 0 or 1\n",
    "- m training examples {(x<sup>(1)</sup>, y<sup>(1)</sup>), ...,x<sup>(m)</sup>, y<sup>(m)</sup>)}\n",
    "- m = m<sub>train</sub>\n",
    "- X = each vector with m columns and number of rows is n (number of features), revere from what is usually done (Xshape) = (n, m)\n",
    "- Y is also in columns Y :Shape = (1,m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given x , want y = P(y=1|x)\n",
    "- parameters with w, and b, how to generate y hat\n",
    "- yhat = wx + b\n",
    "- yhat should be between 0 and 1\n",
    "- y = sigmoid function(wx + b)\n",
    "- z = wx + b\n",
    "- $$G(x) = \\frac{1}{1+e^{-z}}$$\n",
    "- then e^-z will be 0 if z is large\n",
    "- then z large negative number, then e to huge number, then is 0\n",
    "- note that b and w are separate parameters, ignore theya notations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression Cost Function\n",
    "\n",
    "- given training examples, want w and b so you can get yhat that are similar to the ground truth labels\n",
    "- superscript i is the ith example\n",
    "- Loss(error) function,measures how good it is, use squared error, but optimization does not work with many local optima\n",
    "- Instead use:\n",
    "-  $$L(\\hat{y}, y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))$$\n",
    "- If y = 1, then loss function is first term, want yhat to be large\n",
    "- if y = 0 then loss function if log(1-hat) , want this larger, make yhat small\n",
    "- Loss function is only for one example, need a cost function\n",
    "- Cost function\n",
    "- $$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y^{i}},y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m}[(y^{i}log\\hat{y^{i}} + (1-y^{i})log(1-\\hat{y^{i}}))]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "- WAnt to find w, b that minimizes J\n",
    "- this is a convex function huge reason to use for logisitc regression\n",
    "- try an initial value for w and b, takes a step in the stpepest downhill direction\n",
    "- repeatedly update the value of w\n",
    "- $$w := w - \\alpha\\frac{dJ(w)}{dw}$$\n",
    "- alpha is the learning weight, derivative \"dw\" is the derivative term\n",
    "- $$w := w - \\alpha\\frac{dJ(w,b)}{dw}$$\n",
    "- $$b := b - \\alpha\\frac{dJ(w,b)}{db}$$\n",
    "- need to use partial derivative derivatives if there multivariables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "- if J is function of a,b, c\n",
    "\n",
    "<img src=\"files/Desktop/Screen Shot 2017-09-11 at 4.56.57 PM.png\", width=500, height=500>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivative with computation Graph\n",
    "\n",
    "- $$\\frac{dJ}{dv} = 33.003$$\n",
    "- If you want to determine DJ/dV, can just look at J\n",
    "- what if we want Dj/da\n",
    "- $$\\frac{dJ}{da} = 33.003$$\n",
    "- when you ump up a, you first increase v, and then change in v will cause J to increase\n",
    "- $$\\frac{dJ}{dv}\\frac{dJ}{da}\\frac{dJ}{du} = 3 x 1$$\n",
    "- $$\\frac{dJ}{db} = \\frac{dJ}{du}\\frac{du}{db} =6$$\n",
    "- $$\\frac{dJ}{dc} = \\frac{dJ}{du}\\frac{du}{dc} =9$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Gradient Descent\n",
    "- Summary of logistic regression\n",
    "$$z= w^{T}x + b$$\n",
    "$$ \\hat{y} = a = \\sigma(z)$$ \n",
    "$$L(\\hat{y}, y) = -(ylog\\hat{y} + (1-y)log(1-\\hat{y}))$$\n",
    "- lets say we have two features xi and x2 \n",
    "- first compute z, then y hat by applying the tranformation, and then getting the loss function\n",
    "- want to modify parameters to modify the loss \n",
    "$$\\frac{dL}{da} = \\frac{-y}{a} + \\frac{1-y}{1-a}$$\n",
    "$$\\frac{dL}{dz} = dz=  a-y$$\n",
    "$$\\frac{dL}{dw1} = x1dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent on m Examples\n",
    "- - Cost function\n",
    "- $$J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}L(\\hat{y^{i}},y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m}[(y^{i}log\\hat{y^{i}} + (1-y^{i})log(1-\\hat{y^{i}}))]$$\n",
    "$$ \\hat{y^{(i)}} = a^{(i)} = \\sigma(z^{(i)}) = w^{T}x^{(i)} + b $$\n",
    "- need to take the derivative to apply gradient descent\n",
    "- $$\\frac{\\partial}{\\partial w1}J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial}{\\partial w1}L(\\hat{y^{i}},y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m}\\frac{\\partial}{\\partial w1}[(y^{i}log\\hat{y^{i}} + (1-y^{i})log(1-\\hat{y^{i}}))]$$\n",
    "- step used to find derivative using computation graph\n",
    "$$\\frac{dL}{dz^{(i)}} = dz^{(i)}=  a^{(i)}-y^{(i)}$$\n",
    "$$\\frac{dL}{dw1} = dw1= x1^{(i)}dz^{(i)}$$ -- note this is cumulative, hence no i subscript on w or b\n",
    "$$\\frac{dL}{db^{(i)}} = db^{(i)}= dz^{(i)}$$\n",
    "\n",
    "- use vectorization to avoid for loops and make code more efficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "- $$z= w^{T}x + b$$\n",
    "- non- vectorized, z=0 for i in range of n, zt. is\n",
    "- use z = np.dot(w,x) +b\n",
    "- CPU versus GPU single installation multiple data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3,4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250073.532828\n",
      "Vectorized version:1.9557476043701172ms\n",
      "250073.532828\n",
      "for loop:530.0121307373047ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "a= np.random.rand(1000000)\n",
    "b = np.random.rand(1000000)\n",
    "\n",
    "tic = time.time()\n",
    "c = np.dot(a,b)\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"Vectorized version:\" + str(1000*(toc-tic)) + \"ms\")\n",
    "\n",
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(1000000):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "print(c)\n",
    "print(\"for loop:\" + str(1000*(toc-tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Vectorization Examples\n",
    "- always avoid explicit for loops\n",
    "- compute u = Av\n",
    "- ui = sum of Ai*vj\n",
    "- also good if you apply exponential operation on every elemnt\n",
    "- import numpy as np\n",
    "u= np.exp(v)\n",
    "- np.log(v)\n",
    "- np.abs(v)\n",
    "- np.maximum(v,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression\n",
    "- X inputs in (nx, m) dimensional matrix\n",
    "- compute z1 z2, z3 in one line of code\n",
    "- wTX + (b, b, b [1 x m) row vector\n",
    "- WT is a (1x n) row vector\n",
    "- then get a z 1x m matrix\n",
    "- z = np.dot(w.T, x) + b (b will get expanded out, called broadcasting in python)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing Logistic Regression Gradient Computation\n",
    "- get each dz for each training example\n",
    "- make a dZ matrix that is 1xm\n",
    "- calculate A and Y\n",
    "- dZ = A - Y\n",
    "- to get dW and dB\n",
    "$$\\frac{dL}{dB} = \\frac{1}{m}\\sum_{i=1}^{m}(dz)^{i}$$\n",
    "$$\\frac{dL}{dB} = \\frac{1}{m}np.sum(dZ)$$\n",
    "$$ dW = \\frac{1}{m}Xdz^{T}$$\n",
    "$$ dW = \\frac{1}{m}[x^{(1)}dz^{(1)} + ... + x^{(m)}dz^{(m)}]$$\n",
    "- this is a nx1 vector\n",
    "- Z = np.dot(w.T, X) + b\n",
    "- A = sigmoid(Z)\n",
    "- dZ = A-Y\n",
    "- $$dB = \\frac{1}{m}np.sum(dZ)$$\n",
    "- $$ dW = \\frac{1}{m}Xdz^{T}$$\n",
    "\n",
    "- $$w := w - \\alpha dw$$\n",
    "- $$b := b - \\alpha db$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting in Python\n",
    "- sum each column of matrix to get totals, and divide throughout matrix to get percentage of calories from all food\n",
    "- can you do this without using a for loop\n",
    "- matrix A (3,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  56.     0.     4.4   68. ]\n",
      " [   1.2  104.    52.     8. ]\n",
      " [   1.8  135.    99.     0.9]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[56, 0, 4.4, 68],\n",
    "            [1.2, 104, 52, 8],\n",
    "            [1.8, 135, 99, 0.9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  59.   239.   155.4   76.9]\n"
     ]
    }
   ],
   "source": [
    "cal = A.sum(axis =0)\n",
    "print(cal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 94.91525424   0.           2.83140283  88.42652796]\n",
      " [  2.03389831  43.51464435  33.46203346  10.40312094]\n",
      " [  3.05084746  56.48535565  63.70656371   1.17035111]]\n"
     ]
    }
   ],
   "source": [
    "percentage = 100*A/cal.reshape(1,4) #divided a 3 by 4 by a 1x4\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Broadcast repeats\n",
    "- if you have (2 x 3) matrix and add to a 1x3 matrix will copy it 2 times in order to add to each row\n",
    "- if you have (m x n) and you have (mx 1) will copy n times to add to each row\n",
    "- if you have (m, n) matrix and you add substract, multiply or divide, and you have (1,n) then it will copy to make it (m,n), same applies if you have (m,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and tricks for python\n",
    "- a= np.random.randn(5) makes random array that does not work as row vector DO NOT USE\n",
    "- INSTEAD do a = np.random.randn(5,1) # more like a column vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05305301  0.66900081 -2.22449788 -0.70951686  0.26684431]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.05305301  0.66900081 -2.22449788 -0.70951686  0.26684431]\n"
     ]
    }
   ],
   "source": [
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.07949362875\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09316437]\n",
      " [ 0.21954029]\n",
      " [ 0.40395465]\n",
      " [ 1.76514154]\n",
      " [ 0.28133451]]\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(5,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.09316437  0.21954029  0.40395465  1.76514154  0.28133451]]\n"
     ]
    }
   ],
   "source": [
    "print(a.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0086796  -0.02045333 -0.03763418 -0.1644483  -0.02621035]\n",
      " [-0.02045333  0.04819794  0.08868432  0.38751968  0.06176426]\n",
      " [-0.03763418  0.08868432  0.16317936  0.71303714  0.11364638]\n",
      " [-0.1644483   0.38751968  0.71303714  3.11572464  0.49659522]\n",
      " [-0.02621035  0.06176426  0.11364638  0.49659522  0.0791491 ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(a, a.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "- runs on a kernel, may need to restart the kernel "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
